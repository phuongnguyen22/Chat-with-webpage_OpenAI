{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e850a46f-6b7c-4427-99bd-c68ab8b8f631",
   "metadata": {},
   "source": [
    "## 0. Load requirement packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad9b918-d969-4464-b73c-1caa208ccc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94fa3a7-5cca-410a-821e-c9a9400605dc",
   "metadata": {},
   "source": [
    "## 1. Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea5b13bc-097d-4f77-80b9-fd5ca84fdd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables, i.e. OPENAI_API_KEY\n",
    "load_dotenv(r\"C:\\Users\\ThiLanPhuongNguyen\\OneDrive - Quant Decisions S.L\\Projects\\web_scraping2\\env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098c990-6180-4b65-ba3c-3d25e64dde65",
   "metadata": {},
   "source": [
    "## 2. Initialize the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1568102-7a69-4f17-b5cb-a95ce6ceae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model, i.e. gpt-3.5-turbo\n",
    "#llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60768f31-e422-44e8-9b4c-f2f30db8324d",
   "metadata": {},
   "source": [
    "## 3. Define prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47e01297-f7e2-44d7-bb9b-03393043174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the chatbot\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template for the chatbot\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fff497-e078-4ad5-a265-ebf2a154961e",
   "metadata": {},
   "source": [
    "## 4. Load webpage and create Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4e3fd80-2061-4b7b-8c9d-4104bf2bc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(url_webpage):\n",
    "    \"\"\"\n",
    "    This function loads a webpage, splits the text, creates a VectorStore, and creates a LangChain.\n",
    "\n",
    "    Parameters:\n",
    "    url_webpage (str): The URL of the webpage to load.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating that the webpage was loaded successfully.\n",
    "    \"\"\"\n",
    "    # Load the webpage\n",
    "    loader = WebBaseLoader(url_webpage)\n",
    "\n",
    "    # Split the text from the webpage\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(loader.load())\n",
    "\n",
    "    # Create a VectorStore from the splits\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "    # Create a Retriever from the VectorStore\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "retriever = load_page(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df37db1-1413-46c9-9be3-d6c09c1e3c72",
   "metadata": {},
   "source": [
    "## 5. Define chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6129ddfb-0bcb-4f24-ba5b-c48b0cc34b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangChain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde7565-8273-467f-95f2-aeb3d0d0540e",
   "metadata": {},
   "source": [
    "## 6. Question & Answer (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b144821e-b65d-4bbc-b5b1-c933034baf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm_response(question):\n",
    "    # Generate a response to the message\n",
    "    llm_response = rag_chain.invoke(question)\n",
    "\n",
    "    return  llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9100d0f-7f67-4e71-9c5e-6960a73eb2a6",
   "metadata": {},
   "source": [
    "### QA #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87de9db1-ff91-4c2a-9bc1-24986a309d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The topic of the blog post is \"LLM Powered Autonomous Agents.\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question example\n",
    "question = \"What is the topic of the blog post?\"\n",
    "make_llm_response(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b939ded-3016-4399-bbc9-b3bb1bf20684",
   "metadata": {},
   "source": [
    "### QA #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c004a13-d9d8-47e2-a9d2-d7a5bb9d818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The concept of building agents with LLM as the core controller is innovative and promising. Proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI showcase its potential beyond generating content to being a powerful problem solver in autonomous agent systems.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question example\n",
    "question = \"Give me a concise and short summary\"\n",
    "make_llm_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d4404-e7d1-48e7-9134-967b1e79474a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
